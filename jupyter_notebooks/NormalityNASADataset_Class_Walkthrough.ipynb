{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de92a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a640bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a list that represents the data in a DataFrame back \n",
    "# to a DataFrame object.\n",
    "def NASA_dataset_window_slice_to_dfs(list_df):\n",
    "    col_names = [\"unit\",\"cycle\",\"operational_setting_1\",\"operational_setting_2\",\n",
    "                 \"operational_setting_3\",\"sensor_measurement_1\",\n",
    "                 \"sensor_measurement_2\",\"sensor_measurement_3\",\n",
    "                 \"sensor_measurement_4\",\"sensor_measurement_5\",\n",
    "                 \"sensor_measurement_6\",\"sensor_measurement_7\",\n",
    "                 \"sensor_measurement_8\",\"sensor_measurement_9\",\n",
    "                 \"sensor_measurement_10\",\"sensor_measurement_11\",\n",
    "                 \"sensor_measurement_12\",\"sensor_measurement_13\",\n",
    "                 \"sensor_measurement_14\",\"sensor_measurement_15\",\n",
    "                 \"sensor_measurement_16\",\"sensor_measurement_17\",\n",
    "                 \"sensor_measurement_18\",\"sensor_measurement_19\",\n",
    "                 \"sensor_measurement_20\",\"sensor_measurement_21\",\"RUL\"]\n",
    "    return pd.DataFrame(list_df, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe4cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalityNASADataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_data_dir_path: str,\n",
    "                 dataset_csv_name: str,\n",
    "                 window_length: int,\n",
    "                 normality_length: int,\n",
    "                 normalize: bool = False,\n",
    "                 DEBUGGING: bool = False):\n",
    "\n",
    "        if DEBUGGING:\n",
    "            print(\"\\nNASADataset object in DEBUGGING mode.\")\n",
    "        \n",
    "        assert window_length < normality_length, \"window_lenght={} must be less\".format(window_length)\\\n",
    "            + \" than normality_lenght={}\".format(normality_length)\n",
    "\n",
    "        self.__DEBUGGING__ = DEBUGGING\n",
    "        self.normalize = normalize\n",
    "        self.window_length = window_length\n",
    "        self.normality_length = normality_length\n",
    "        self.path_to_dataset = os.path.join(root_data_dir_path, dataset_csv_name)\n",
    "\n",
    "        # In order to make use of the .csv files, we must create the data based on the window_length variable.\n",
    "        # The data will be stored as a JSON file. Check if this JSON file exists, if not, create\n",
    "        # and save the file. This can be a computationally expensive process, thus the need to save it as a JSON file.\n",
    "        # We only want the dataset name \"FD00X\", so take the first five chars [:5]\n",
    "        dataset_name = \"normalized_{}\".format(dataset_csv_name[:5]) if self.normalize else dataset_csv_name[:5]\n",
    "\n",
    "        if self.__DEBUGGING__:\n",
    "            JSON_file_name = \"{}_{}_window{}_normalityLen{}.json\".format(\"DEBUGGING\", \n",
    "                                                                         dataset_name, \n",
    "                                                                         self.window_length, \n",
    "                                                                         self.normality_length)\n",
    "        else:\n",
    "            JSON_file_name = \"{}_window{}_normalityLen{}.json\".format(dataset_name, \n",
    "                                                                      self.window_length, \n",
    "                                                                      self.normality_length)\n",
    "\n",
    "        # Define the directory where all JSON files will be stored.\n",
    "        all_JSONs_files_dir_path = os.path.join(root_data_dir_path, \"JSON_data\")\n",
    "        JSON_file_path = os.path.join(all_JSONs_files_dir_path, JSON_file_name)\n",
    "\n",
    "        # If NOT exists, CREATE the data\n",
    "        if not os.path.exists(JSON_file_path):\n",
    "            if not os.path.exists(all_JSONs_files_dir_path):\n",
    "                os.makedirs(all_JSONs_files_dir_path)\n",
    "            # Read data files only if the JSON file does not exist. These are big files so no need to read\n",
    "            # them if there is no need.\n",
    "            print(\"JSON file '{}' does not exist at {}. \\n\\nCreating JSON file...\".format(JSON_file_name,\n",
    "                                                                                          JSON_file_path))\n",
    "            df_dataset: pd.DataFrame = pd.read_csv(self.path_to_dataset)\n",
    "\n",
    "            # Normalize all columns in the DataFrame\n",
    "            if self.normalize:\n",
    "                df_dataset.iloc[:, 2:-1] = (df_dataset.iloc[:, 2:-1] - df_dataset.iloc[:, 2:-1].mean()) / \\\n",
    "                                           (df_dataset.iloc[:, 2:-1].std() + 1)\n",
    "\n",
    "            # Define the column in the DataFrame that identifies every unit in the dataset\n",
    "            unit_identifier_col_name = \"unit\"\n",
    "            dict_all_samples_of_window_length = self.__create_JSON_from_window_size__(\n",
    "                df_dataset=df_dataset,\n",
    "                unit_identifier_col_name=unit_identifier_col_name,\n",
    "                path_to_json_file=JSON_file_path\n",
    "            )\n",
    "        # Else, LOAD the data\n",
    "        else:\n",
    "            # Read file\n",
    "            print(\"Reading {} JSON file at: \\n{}\\n\".format(JSON_file_name, all_JSONs_files_dir_path))\n",
    "            with open(JSON_file_path, 'r') as file:\n",
    "                dict_all_samples_of_window_length = json.load(file)\n",
    "\n",
    "        self.data = dict_all_samples_of_window_length[\"samples\"]\n",
    "        self.size = len(self.data)\n",
    "\n",
    "        print(\"{} total samples generated with normality_length={} and window_length={}.\"\\\n",
    "              .format(self.size,\n",
    "                      self.normality_length,\n",
    "                      self.window_length))\n",
    "\n",
    "    def __create_JSON_from_window_size__(self,\n",
    "                                         df_dataset: pd.DataFrame,\n",
    "                                         unit_identifier_col_name: str,\n",
    "                                         path_to_json_file: str):\n",
    "\n",
    "        # Get a list that contains the unique units in the list\n",
    "        all_unique_units_list: list = df_dataset[unit_identifier_col_name].unique().tolist()\n",
    "\n",
    "        # Iterate over all units and get their samples of window_length and save to list. Save them to a list\n",
    "        all_samples_list: list = []\n",
    "        for i, unit in enumerate(all_unique_units_list):\n",
    "            unit_samples_list = self.__get_samples_for_unit__(df_dataset,\n",
    "                                                              unit=unit,\n",
    "                                                              unit_identifier_col_name=unit_identifier_col_name)\n",
    "            all_samples_list += unit_samples_list\n",
    "\n",
    "            print(\"Number of samples in unit {}: {}\".format(unit, len(unit_samples_list)))\n",
    "            if self.__DEBUGGING__ and i == 0: break\n",
    "\n",
    "        # Save the data\n",
    "        dict_all_samples_of_window_length: dict = {\n",
    "            \"samples\": all_samples_list,\n",
    "        }\n",
    "\n",
    "        json_object = json.dumps(dict_all_samples_of_window_length)\n",
    "        with open(path_to_json_file, 'w') as file:\n",
    "            file.write(json_object)\n",
    "\n",
    "        print(\"\\nFinished writing JSON file to: \\n{}.\\n\".format(path_to_json_file))\n",
    "\n",
    "        return dict_all_samples_of_window_length\n",
    "\n",
    "    def __get_samples_for_unit__(self,\n",
    "                                 df_dataset: pd.DataFrame,\n",
    "                                 unit: str,\n",
    "                                 unit_identifier_col_name: str):\n",
    "\n",
    "        # Get the subset DataFrame for a unique unit\n",
    "        df_dataset_for_unit = df_dataset.query(\"{} == {}\".format(unit_identifier_col_name, unit))\n",
    "\n",
    "        # Adjust the total number of samples by making it only the lenght of the normality range\n",
    "        # tot_num_samples = df_dataset_for_unit.shape[0]\n",
    "        tot_num_samples = self.normality_length\n",
    "       \n",
    "\n",
    "        # Perform basic error checking\n",
    "        if self.window_length > tot_num_samples:\n",
    "            error = \"Window (window_size={}) must  be less than the number of total samples \" \\\n",
    "                    \"(unit_number_total_samples={}) for unit={}\"\\\n",
    "                .format(self.window_length, tot_num_samples, unit)\n",
    "            raise RuntimeError(error)\n",
    "        if self.window_length <= 0:\n",
    "            error = \"window_size must be greater than 0\"\n",
    "            raise RuntimeError(error)\n",
    "\n",
    "        # Compute the number of samples of size window_size that will be obtained form this unit.\n",
    "        # Since we are using the sliding window approach, this can easily be computed.\n",
    "        number_sliding_window_samples = tot_num_samples - self.window_length + 1\n",
    "\n",
    "        # We start from the first cycle, which is at index=0\n",
    "        list_all_samples = []\n",
    "        for start_cycle_index in range(0, number_sliding_window_samples):\n",
    "            # Only create window-slices of length window_length up until the normality_length\n",
    "            # if start_cycle_index < self.normality_length:\n",
    "            \n",
    "            # Get the window as a dataframe (This may not be optimal since we are indexing a DataFrame)\n",
    "            # Consider changing this to a 2D array?\n",
    "            df_window = df_dataset_for_unit.iloc[start_cycle_index:start_cycle_index + self.window_length, :]\n",
    "\n",
    "            # Save the window as a list\n",
    "            list_all_samples.append(df_window.values.tolist())\n",
    "        return list_all_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Every sample in the data array should have the following structure:\n",
    "        # [\n",
    "        #   \"unit\",\"cycle\",\"operational_setting_1\",\"operational_setting_2\",\"operational_setting_3\",\n",
    "        #   \"sensor_measurement_1\", \"sensor_measurement_2\",\"sensor_measurement_3\",\"sensor_measurement_4\",\n",
    "        #   \"sensor_measurement_5\",\"sensor_measurement_6\", \"sensor_measurement_7\",\"sensor_measurement_8\",\n",
    "        #   \"sensor_measurement_9\",\"sensor_measurement_10\", \"sensor_measurement_11\", \"sensor_measurement_12\",\n",
    "        #   \"sensor_measurement_13\",\"sensor_measurement_14\", \"sensor_measurement_15\",\"sensor_measurement_16\",\n",
    "        #   \"sensor_measurement_17\",\"sensor_measurement_18\", \"sensor_measurement_19\",\"sensor_measurement_20\",\n",
    "        #   \"sensor_measurement_21\",\"RUL\"\n",
    "        # ]\n",
    "        # Where the first two columns represent the unit and cycle, respectively.\n",
    "        window = np.array(self.data[idx], dtype=np.float32)\n",
    "\n",
    "        # Create the features used for training and set as float32.\n",
    "        # Also create the labels (RUL) which include system name for labeling purposes.\n",
    "        # Features are all columns except the first two (Unit and cycle) and last one (RUL),\n",
    "        # RUL = last column\n",
    "        unit = window[:, 0][0].astype(int)\n",
    "        features = window[:, 2:-1]\n",
    "        rul = window[:, -1][-1]\n",
    "\n",
    "        return unit, features, rul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a28af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: \n",
      "/Users/rafaeltoche/Documents/School/Research/Rainwaters_Lab/DART-LP2/NASA_Turbofan/NASA_turbofan_data/train/FD001_train.csv\n"
     ]
    }
   ],
   "source": [
    "root_data_dir_path = \"/Users/rafaeltoche/Documents/School/Research/Rainwaters_Lab/\"\\\n",
    "                        \"DART-LP2/NASA_Turbofan/NASA_turbofan_data/train\"\n",
    "csv_dataset_name = \"FD001_train.csv\"\n",
    "window_length = 10\n",
    "normality_length = 20\n",
    "print(\"Reading data from: \\n{}\".format(os.path.join(root_data_dir_path, csv_dataset_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96923fa",
   "metadata": {},
   "source": [
    "### Testing with Only one unit\n",
    "DEBUGGING=TRUE \n",
    "\n",
    "set i = 0 in `__create_JSON_from_window_size__` function to get only **one** unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38954a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA_dataset = NormalityNASADataset(root_data_dir_path=root_data_dir_path, \n",
    "#                                     dataset_csv_name=csv_dataset_name, \n",
    "#                                     window_length=window_length, \n",
    "#                                     normality_length=normality_length,\n",
    "#                                     DEBUGGING=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732d72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoader\n",
    "# dataloader = DataLoader(NASA_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1fdf8",
   "metadata": {},
   "source": [
    "View the first instance of the window data for this one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9208bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA_dataset_window_slice_to_dfs(dataloader.dataset.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0dd1f",
   "metadata": {},
   "source": [
    "View the last instance of the window data for this one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced4c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA_dataset_window_slice_to_dfs(dataloader.dataset.data[len(dataloader.dataset.data) -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a943ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_unit_ids, train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec1d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Unit IDs Shape: {}\".format(train_unit_ids.shape))\n",
    "# print(\"Features Shape: {}\".format(train_features.shape))\n",
    "# print(\"Labels Shape: {}\\n\".format(train_labels.shape))\n",
    "\n",
    "# print(\"Unit IDs Dtype: {}\".format(train_unit_ids.dtype))\n",
    "# print(\"Features Dtype: {}\".format(train_features.dtype))\n",
    "# print(\"Labels Dtype: {}\".format(train_labels.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8851a6",
   "metadata": {},
   "source": [
    "### Testing with more than one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78cf4139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'FD001_window10_normalityLen20.json' does not exist at /Users/rafaeltoche/Documents/School/Research/Rainwaters_Lab/DART-LP2/NASA_Turbofan/NASA_turbofan_data/train/JSON_data/FD001_window10_normalityLen20.json. \n",
      "\n",
      "Creating JSON file...\n",
      "Number of samples in unit 1: 11\n",
      "Number of samples in unit 2: 11\n",
      "Number of samples in unit 3: 11\n",
      "Number of samples in unit 4: 11\n",
      "Number of samples in unit 5: 11\n",
      "Number of samples in unit 6: 11\n",
      "Number of samples in unit 7: 11\n",
      "Number of samples in unit 8: 11\n",
      "Number of samples in unit 9: 11\n",
      "Number of samples in unit 10: 11\n",
      "Number of samples in unit 11: 11\n",
      "Number of samples in unit 12: 11\n",
      "Number of samples in unit 13: 11\n",
      "Number of samples in unit 14: 11\n",
      "Number of samples in unit 15: 11\n",
      "Number of samples in unit 16: 11\n",
      "Number of samples in unit 17: 11\n",
      "Number of samples in unit 18: 11\n",
      "Number of samples in unit 19: 11\n",
      "Number of samples in unit 20: 11\n",
      "Number of samples in unit 21: 11\n",
      "Number of samples in unit 22: 11\n",
      "Number of samples in unit 23: 11\n",
      "Number of samples in unit 24: 11\n",
      "Number of samples in unit 25: 11\n",
      "Number of samples in unit 26: 11\n",
      "Number of samples in unit 27: 11\n",
      "Number of samples in unit 28: 11\n",
      "Number of samples in unit 29: 11\n",
      "Number of samples in unit 30: 11\n",
      "Number of samples in unit 31: 11\n",
      "Number of samples in unit 32: 11\n",
      "Number of samples in unit 33: 11\n",
      "Number of samples in unit 34: 11\n",
      "Number of samples in unit 35: 11\n",
      "Number of samples in unit 36: 11\n",
      "Number of samples in unit 37: 11\n",
      "Number of samples in unit 38: 11\n",
      "Number of samples in unit 39: 11\n",
      "Number of samples in unit 40: 11\n",
      "Number of samples in unit 41: 11\n",
      "Number of samples in unit 42: 11\n",
      "Number of samples in unit 43: 11\n",
      "Number of samples in unit 44: 11\n",
      "Number of samples in unit 45: 11\n",
      "Number of samples in unit 46: 11\n",
      "Number of samples in unit 47: 11\n",
      "Number of samples in unit 48: 11\n",
      "Number of samples in unit 49: 11\n",
      "Number of samples in unit 50: 11\n",
      "Number of samples in unit 51: 11\n",
      "Number of samples in unit 52: 11\n",
      "Number of samples in unit 53: 11\n",
      "Number of samples in unit 54: 11\n",
      "Number of samples in unit 55: 11\n",
      "Number of samples in unit 56: 11\n",
      "Number of samples in unit 57: 11\n",
      "Number of samples in unit 58: 11\n",
      "Number of samples in unit 59: 11\n",
      "Number of samples in unit 60: 11\n",
      "Number of samples in unit 61: 11\n",
      "Number of samples in unit 62: 11\n",
      "Number of samples in unit 63: 11\n",
      "Number of samples in unit 64: 11\n",
      "Number of samples in unit 65: 11\n",
      "Number of samples in unit 66: 11\n",
      "Number of samples in unit 67: 11\n",
      "Number of samples in unit 68: 11\n",
      "Number of samples in unit 69: 11\n",
      "Number of samples in unit 70: 11\n",
      "Number of samples in unit 71: 11\n",
      "Number of samples in unit 72: 11\n",
      "Number of samples in unit 73: 11\n",
      "Number of samples in unit 74: 11\n",
      "Number of samples in unit 75: 11\n",
      "Number of samples in unit 76: 11\n",
      "Number of samples in unit 77: 11\n",
      "Number of samples in unit 78: 11\n",
      "Number of samples in unit 79: 11\n",
      "Number of samples in unit 80: 11\n",
      "Number of samples in unit 81: 11\n",
      "Number of samples in unit 82: 11\n",
      "Number of samples in unit 83: 11\n",
      "Number of samples in unit 84: 11\n",
      "Number of samples in unit 85: 11\n",
      "Number of samples in unit 86: 11\n",
      "Number of samples in unit 87: 11\n",
      "Number of samples in unit 88: 11\n",
      "Number of samples in unit 89: 11\n",
      "Number of samples in unit 90: 11\n",
      "Number of samples in unit 91: 11\n",
      "Number of samples in unit 92: 11\n",
      "Number of samples in unit 93: 11\n",
      "Number of samples in unit 94: 11\n",
      "Number of samples in unit 95: 11\n",
      "Number of samples in unit 96: 11\n",
      "Number of samples in unit 97: 11\n",
      "Number of samples in unit 98: 11\n",
      "Number of samples in unit 99: 11\n",
      "Number of samples in unit 100: 11\n",
      "\n",
      "Finished writing JSON file to: \n",
      "/Users/rafaeltoche/Documents/School/Research/Rainwaters_Lab/DART-LP2/NASA_Turbofan/NASA_turbofan_data/train/JSON_data/FD001_window10_normalityLen20.json.\n",
      "\n",
      "1100 total samples generated with normality_length=20 and window_length=10.\n"
     ]
    }
   ],
   "source": [
    "NASA_dataset = NormalityNASADataset(root_data_dir_path=root_data_dir_path, \n",
    "                                    dataset_csv_name=csv_dataset_name, \n",
    "                                    window_length=window_length, \n",
    "                                    normality_length=normality_length,\n",
    "                                    DEBUGGING=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d343a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 880\n",
      "Test Size: 220\n"
     ]
    }
   ],
   "source": [
    "split = 0.80\n",
    "train_size = math.ceil(NASA_dataset.size * split)\n",
    "test_size = NASA_dataset.size - train_size\n",
    "\n",
    "print(\"Train Size: {}\".format(train_size))\n",
    "print(\"Test Size: {}\".format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0234b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test split\n",
    "train_dataset, test_dataset = random_split(NASA_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8538c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoaders\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0032c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit IDs Shape: torch.Size([5])\n",
      "Features Shape: torch.Size([5, 10, 24])\n",
      "Labels Shape: torch.Size([5])\n",
      "\n",
      "Unit IDs Dtype: torch.int64\n",
      "Features Dtype: torch.float32\n",
      "Labels Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_unit_ids, train_features, train_RUL_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(\"Unit IDs Shape: {}\".format(train_unit_ids.shape))\n",
    "print(\"Features Shape: {}\".format(train_features.shape))\n",
    "print(\"Labels Shape: {}\\n\".format(train_RUL_labels.shape))\n",
    "\n",
    "print(\"Unit IDs Dtype: {}\".format(train_unit_ids.dtype))\n",
    "print(\"Features Dtype: {}\".format(train_features.dtype))\n",
    "print(\"Labels Dtype: {}\".format(train_RUL_labels.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b62ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DART",
   "language": "python",
   "name": "dart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
